# -*- coding: utf-8 -*-
"""data_processing(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13FMWr53KVAaOXu4iFguEnVBpA2EH4NYh

# Загрузка и обработка данных
"""

import pandas as pd
import numpy as np

# Скачиваем данные вручную или через git-lfs
# !curl -o dataset.tskv https://raw.githubusercontent.com/yandex/geo-reviews-dataset-2023/master/geo-reviews-dataset-2023.tskv

# Из Colab можно склонировать репозиторий
!git clone https://github.com/yandex/geo-reviews-dataset-2023.git

def parse_tskv_line(line):
    # Разделяем строку по табуляции и создаём словарь из пар ключ-значение.
    return dict(item.split('=', 1) for item in line.strip().split('\t'))

# Чтение файла строка за строкой и преобразование каждой строки через parse_tskv_line
with open('geo-reviews-dataset-2023/geo-reviews-dataset-2023.tskv', 'r') as file:
    data = [parse_tskv_line(line) for line in file]

# Создание DataFrame из списка словарей
df = pd.DataFrame(data)

# Вывод первых нескольких строк DataFrame
df.head()

# Убираем пустые значения оценок
df = df[df['rating'] != '0.']

# Делаем классы от 0 до 4 для удобства обучения
df["label"] = df.rating.apply(lambda x: int(x[0]) - 1)

# Поменяли имя колонки с 'rating' на 'label'
df = df.drop(columns=['rating'])

df.head()

"""# Загрузка dataset"""

! pip install datasets
! pip install -U accelerate
! pip install -U transformers

from datasets import load_dataset, Dataset
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

"""## Dataset Analysis"""

import matplotlib.pyplot as plt

df["label"].value_counts(ascending=True).plot.barh()
plt.title("Frequency of Classes")
plt.show()

df["Words Per Review"] = df["text"].str.split().apply(len)
df.boxplot("Words Per Review", by="label", grid=False,
          showfliers=False, color="black")
plt.suptitle("")
plt.xlabel("")
plt.show()

"""## train/test split"""

from sklearn.model_selection import train_test_split

train, test = train_test_split(df, test_size=0.10, stratify=df.label)

train_rawdataset = Dataset.from_pandas(train)
test_rawdataset = Dataset.from_pandas(test)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
model_name = "DeepPavlov/rubert-base-cased"
tokenizer = BertTokenizer.from_pretrained(model_name, model_max_length=512)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=5).to(device)  # Adjust num_labels for your task

for param in model.parameters () : param.data = param.data.contiguous ()

"""## Токенизация"""

def preprop(sample):
  return tokenizer(sample['text'], truncation=True)

tokenized_train = train_rawdataset.map(preprop, batched=True)
tokenized_test = test_rawdataset.map(preprop, batched=True)

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer", eval_strategy="epoch")

from transformers import Trainer

from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    f1 = f1_score(labels, preds, average="weighted")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1}

batch_size = 16
logging_steps = len(tokenized_train) // batch_size
model_name = (f"{model_name} - finetuned_reviews")
training_args = TrainingArguments(output_dir=model_name,
                                  num_train_epochs=5,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  eval_strategy="epoch",
                                  disable_tqdm=False,
                                  logging_steps=logging_steps,

                                  log_level="error"
                                  )

trainer = Trainer(
    model = model,
    args = training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics

)

torch.cuda.memory_summary(device=None, abbreviated=False)

trainer.train()

path_to_save = f"{model_name} - finetuned_reviews_2"
model.save_pretrained(path_to_save)

from google.colab import drive
drive.mount('/content/drive')

"""# Загрузка тестовых данных и разметка"""

with open('test.tskv', 'r') as file:
    data = [parse_tskv_line(line) for line in file]

df_test = pd.DataFrame(data)
df_test.head()

def predict_rating(row):
    # return str(predict(row['text'], row['address'], row['name_ru'], row['rubrics']) + 1) + '.'
    return '1.'

# Предсказываем классы
df_test['rating'] = df_test.apply(predict_rating, axis=1)
df_test.head()

with open('test_with_rating.tskv', 'w') as file:
    for index, row in df_test.iterrows():
        line = ''
        for column in df_test.columns:
            line += f'{column}={row[column]}\t'
        print(line.strip(), end='\n', file=file)

with open('test_with_rating.tskv', 'r') as file:
    data = [parse_tskv_line(line) for line in file]

df_test = pd.DataFrame(data)
df_test.head()